{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f1a166",
   "metadata": {},
   "source": [
    "# Deep Q Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82f76c0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9697eec7",
   "metadata": {},
   "source": [
    "This notebook reproduce the DeepQLearning network and then use this algorithm to control \"CartPole\" game.\n",
    "\n",
    "Also, this code refers [MorvanZhou](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/5_Deep_Q_Network/DQN_modified.py) and [ljp](https://github.com/ljpzzz/machinelearning/blob/master/reinforcement-learning/dqn.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e73d3a7",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "176993fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0553d7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e6eaa0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eaf8626",
   "metadata": {},
   "source": [
    "## Code Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9948b361",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQLearning:\n",
    "    def __init__(self, n_features, n_actions, learning_rate = 0.01 ,reward_decay = 0.9, e_greedy = 0.9, memory_size = 500, batch_size = 32):\n",
    "        self.n_features = n_features\n",
    "        self.n_actions = n_actions\n",
    "        self.lr = learning_rate\n",
    "        self.gamma = reward_decay\n",
    "        self.epsilon = e_greedy\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self._build_network()\n",
    "        \n",
    "        self._train_network()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.memory = np.zeros((memory_size, 2 * n_features + 2))\n",
    "        self.memory_counter = 0 \n",
    "        \n",
    "        \n",
    "    def _build_network(self):\n",
    "        observations = tf.placeholder(tf.float32, shape = (None, self.n_features))\n",
    "        \n",
    "        tf.random_normal_initializer(0., 0.3), tf.constant_initializer(0.1)\n",
    "        # DNN network\n",
    "        fc1 = tf.layers.dense(observation, 8, tf.nn.relu, \n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.3), \n",
    "                              bias_initializer=tf.constant_initializer(0.1))\n",
    "        self.q_eval = tf.layers.dense(fc1, self.n_actions,  \n",
    "                              kernel_initializer=tf.random_normal_initializer(0., 0.3), \n",
    "                              bias_initializer=tf.constant_initializer(0.1))\n",
    "      \n",
    "    def _train_network(self):\n",
    "        \n",
    "        actions = tf.placeholder(tf.int32, shape = (None,))\n",
    "        \n",
    "        rewards = tf.placeholder(tf.int32, shape = (None,))\n",
    "        \n",
    "        # Q(S',argmax_A(S',A))\n",
    "        q_next = tf.placeholder(tf.float32, shape = (None,))\n",
    "        \n",
    "        hot_code_actions = tf.transpose(tf.one_hot(actions, self.n_actions))\n",
    "        \n",
    "        # Q(S,A)\n",
    "        q_eval = tf.matmul(self.q_eval, hot_code_actions)\n",
    "        \n",
    "        loss = tf.mean_squared_error(labels = q_next + reward, predictions = q_eval)\n",
    "        \n",
    "        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)\n",
    "    \n",
    "    def store_transition(self, s, a, r, s_):\n",
    "        now = np.hstack([s, [a, r], s_])\n",
    "        \n",
    "        if self.memory_counter < self.memory_size:\n",
    "            self.memory[self.memory_counter] = now\n",
    "        else:\n",
    "            index = self.memory_counter % self.memory_size\n",
    "            self.memory[index] = now\n",
    "    \n",
    "    def train(self):\n",
    "        if self.memory_counter > self.memory_size:\n",
    "            sample_index = np.random.choice(self.memory_size, size = self.batch_size)\n",
    "        else:\n",
    "            sample_index = np.random.choice(self.memory_counter, size = self.batch_size)\n",
    "            \n",
    "        batch_memory = self.memory[sample_index, :]\n",
    "        \n",
    "        s = batch_memory[:, :n_features]\n",
    "        \n",
    "        a = batch_memory[:, n_features]\n",
    "        \n",
    "        r = batch_memory[:, n_features+1]\n",
    "        \n",
    "        s_ = batch_memory[:, n_features+1:]\n",
    "                \n",
    "        q_next_eval = np.zeros_like(r)\n",
    "        \n",
    "        eval_sample = []\n",
    "        \n",
    "        for i in range(q_next_eval.shape[0]):\n",
    "            if not True in np.isnan(s_[i]):\n",
    "                eval_sample.append(i)\n",
    "        \n",
    "        # eval the actions\n",
    "        eval_Q = self.sess.run(self.q_eval, feed_dict = {\n",
    "            observations: s_[eval_sample]\n",
    "        })\n",
    "\n",
    "        # choose best actions\n",
    "        \n",
    "        q_next_eval[eval_sample] = np.sum(eval_Q, axis = 1)  \n",
    "       \n",
    "        # learning from data\n",
    "        self.sess.run(self.train_op, feed_dict = {\n",
    "            observations: s,\n",
    "            actions: a,\n",
    "            rewards: r,\n",
    "            q_next: q_next_eval,\n",
    "        })\n",
    "        \n",
    "    def predict(self, s):\n",
    "        \n",
    "        # eplison - greedy\n",
    "        if np.random.uniform() < self.epsilon:\n",
    "            Q = self.sess.run(self.q_eval, feed_dict = {\n",
    "            observations : s[np.newaxis, :]\n",
    "        })\n",
    "            return np.argmax(Q)\n",
    "        else:\n",
    "            return np.random.randint(self.n_actions)\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7727de",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50136009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2,3,4])[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037e0b54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b614b952",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5243d29a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
