{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52b03b09",
   "metadata": {},
   "source": [
    "# Gradient Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637817d",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0571f59c",
   "metadata": {},
   "source": [
    "This project reproduces the policy gradient algorithm and test this preformance in CartPole \n",
    "\n",
    "And this code project refers this [Code](https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/7_Policy_gradient_softmax/RL_brain.py)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693fee32",
   "metadata": {},
   "source": [
    "## Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "c3db2840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "db0e588f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b28dfd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ae0a35",
   "metadata": {},
   "source": [
    "## Code Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "00255fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "4b91769a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# tf.set_random_seed(1)\n",
    "\n",
    "\n",
    "class PolicyGradient:\n",
    "    def __init__(self, n_actions, n_features, learning_rate = 0.01, reward_decay = 0.95):\n",
    "        self.n_actions = n_actions     # 行动维数\n",
    "        self.n_features = n_features   # 特征维数\n",
    "        self.lr = learning_rate   \n",
    "        self.gamma = reward_decay\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "        self.build_network()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self.ep_obs = []\n",
    "        self.ep_acts = []\n",
    "        self.ep_rs = []\n",
    "        \n",
    "    def build_network(self):\n",
    "        self.obs = tf.placeholder(dtype = tf.float32, shape = (None, self.n_features)) # None代表batch_size\n",
    "        self.acts = tf.placeholder(dtype = tf.int32, shape = (None,))\n",
    "        self.vts = tf.placeholder(dtype = tf.float32, shape = (None,))\n",
    "        \n",
    "        # fc1\n",
    "        layer = tf.layers.dense(\n",
    "            inputs=self.obs,\n",
    "            units=10,\n",
    "            activation=tf.nn.tanh,  # tanh activation\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc1'\n",
    "        )\n",
    "        # fc2\n",
    "        tmp = tf.layers.dense(\n",
    "            inputs=layer,\n",
    "            units=self.n_actions,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.random_normal_initializer(mean=0, stddev=0.3),\n",
    "            bias_initializer=tf.constant_initializer(0.1),\n",
    "            name='fc2'\n",
    "        )     \n",
    "                \n",
    "        self.predict_probs = tf.nn.softmax(tmp)\n",
    "        \n",
    "        #loss = tf.losses.log_loss(labels = self.acts, predictions = self.predict_probs[:, 1], weights = self.vts)\n",
    "        neg_log_prob = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=tmp, labels=self.acts)   # this is negative log of chosen action\n",
    "            \n",
    "        #neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits=tmp, labels=tf.one_hot(self.acts, self.n_actions))\n",
    "        loss = tf.reduce_mean(neg_log_prob * self.vts)\n",
    "   \n",
    "        self.train_op = tf.train.AdamOptimizer(self.lr).minimize(loss)    \n",
    "    \n",
    "    def train(self):\n",
    "        discount_rewards = self.discount_reward()\n",
    "        self.sess.run(self.train_op, feed_dict = {\n",
    "            self.obs: np.vstack(self.ep_obs),\n",
    "            self.acts: self.ep_acts,\n",
    "            self.vts: discount_rewards,\n",
    "        })\n",
    "        self.ep_obs = []\n",
    "        self.ep_acts = []\n",
    "        self.ep_rs = []\n",
    "    \n",
    "    def predict(self, observation):\n",
    "        probs = self.sess.run(self.predict_probs, feed_dict = {self.obs: observation[np.newaxis, :]})\n",
    "        return np.random.choice(list(range(self.n_actions)), p = probs.ravel())\n",
    "    \n",
    "    def store_transition(self, o, a, t):\n",
    "        self.ep_obs.append(o)\n",
    "        self.ep_acts.append(a)\n",
    "        self.ep_rs.append(t)\n",
    "        \n",
    "    def discount_reward(self):\n",
    "        discount_rewards = np.zeros_like(self.ep_rs)\n",
    "        tmp = 0\n",
    "        for i in reversed(range(len(self.ep_rs))):\n",
    "            discount_rewards[i] = self.gamma * tmp + self.ep_rs[i]\n",
    "            tmp = discount_rewards[i]\n",
    "        \n",
    "        discount_rewards -= np.mean(discount_rewards)\n",
    "        discount_rewards /= np.std(discount_rewards)\n",
    "        \n",
    "        return discount_rewards\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f76600",
   "metadata": {},
   "source": [
    "## Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "885753c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "e891d5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON_NUM = 10000\n",
    "THRESHOLD = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ec769cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "# env.seed(1)     \n",
    "env = env.unwrapped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "576904f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "174d8ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolicyGradient(n_actions = env.action_space.n, n_features = env.observation_space.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "3ee4fe80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon:0, reward:18.0\n",
      "epsilon:1, reward:27.0\n",
      "epsilon:2, reward:11.0\n",
      "epsilon:3, reward:18.0\n",
      "epsilon:4, reward:25.0\n",
      "epsilon:5, reward:56.0\n",
      "epsilon:6, reward:33.0\n",
      "epsilon:7, reward:35.0\n",
      "epsilon:8, reward:58.0\n",
      "epsilon:9, reward:23.0\n",
      "epsilon:10, reward:14.0\n",
      "epsilon:11, reward:142.0\n",
      "epsilon:12, reward:45.0\n",
      "epsilon:13, reward:43.0\n",
      "epsilon:14, reward:49.0\n",
      "epsilon:15, reward:13.0\n",
      "epsilon:16, reward:34.0\n",
      "epsilon:17, reward:14.0\n",
      "epsilon:18, reward:29.0\n",
      "epsilon:19, reward:81.0\n",
      "epsilon:20, reward:56.0\n",
      "epsilon:21, reward:27.0\n",
      "epsilon:22, reward:53.0\n",
      "epsilon:23, reward:27.0\n",
      "epsilon:24, reward:19.0\n",
      "epsilon:25, reward:27.0\n",
      "epsilon:26, reward:43.0\n",
      "epsilon:27, reward:68.0\n",
      "epsilon:28, reward:45.0\n",
      "epsilon:29, reward:73.0\n",
      "epsilon:30, reward:33.0\n",
      "epsilon:31, reward:19.0\n",
      "epsilon:32, reward:55.0\n",
      "epsilon:33, reward:36.0\n",
      "epsilon:34, reward:24.0\n",
      "epsilon:35, reward:69.0\n",
      "epsilon:36, reward:41.0\n",
      "epsilon:37, reward:60.0\n",
      "epsilon:38, reward:32.0\n",
      "epsilon:39, reward:140.0\n",
      "epsilon:40, reward:92.0\n",
      "epsilon:41, reward:92.0\n",
      "epsilon:42, reward:103.0\n",
      "epsilon:43, reward:67.0\n",
      "epsilon:44, reward:128.0\n",
      "epsilon:45, reward:84.0\n",
      "epsilon:46, reward:28.0\n",
      "epsilon:47, reward:204.0\n",
      "epsilon:48, reward:100.0\n",
      "epsilon:49, reward:53.0\n",
      "epsilon:50, reward:59.0\n",
      "epsilon:51, reward:38.0\n",
      "epsilon:52, reward:55.0\n",
      "epsilon:53, reward:48.0\n",
      "epsilon:54, reward:169.0\n",
      "epsilon:55, reward:132.0\n",
      "epsilon:56, reward:108.0\n",
      "epsilon:57, reward:77.0\n",
      "epsilon:58, reward:68.0\n",
      "epsilon:59, reward:206.0\n",
      "epsilon:60, reward:174.0\n",
      "epsilon:61, reward:51.0\n",
      "epsilon:62, reward:55.0\n",
      "epsilon:63, reward:185.0\n",
      "epsilon:64, reward:34.0\n",
      "epsilon:65, reward:101.0\n",
      "epsilon:66, reward:123.0\n",
      "epsilon:67, reward:76.0\n",
      "epsilon:68, reward:26.0\n",
      "epsilon:69, reward:114.0\n",
      "epsilon:70, reward:273.0\n",
      "epsilon:71, reward:96.0\n",
      "epsilon:72, reward:264.0\n",
      "epsilon:73, reward:365.0\n",
      "epsilon:74, reward:327.0\n",
      "epsilon:75, reward:114.0\n",
      "epsilon:76, reward:167.0\n",
      "epsilon:77, reward:48.0\n",
      "epsilon:78, reward:30.0\n",
      "epsilon:79, reward:55.0\n",
      "epsilon:80, reward:125.0\n",
      "epsilon:81, reward:140.0\n",
      "epsilon:82, reward:135.0\n",
      "epsilon:83, reward:185.0\n",
      "epsilon:84, reward:204.0\n",
      "epsilon:85, reward:144.0\n",
      "epsilon:86, reward:219.0\n",
      "epsilon:87, reward:202.0\n",
      "epsilon:88, reward:305.0\n",
      "epsilon:89, reward:129.0\n",
      "epsilon:90, reward:624.0\n",
      "epsilon:91, reward:664.0\n",
      "epsilon:92, reward:85.0\n",
      "epsilon:93, reward:507.0\n",
      "epsilon:95, reward:1877.0\n",
      "epsilon:96, reward:180.0\n",
      "epsilon:97, reward:312.0\n",
      "epsilon:98, reward:96.0\n",
      "epsilon:99, reward:508.0\n",
      "epsilon:100, reward:888.0\n",
      "epsilon:101, reward:307.0\n",
      "epsilon:102, reward:479.0\n",
      "epsilon:103, reward:314.0\n",
      "epsilon:104, reward:347.0\n",
      "epsilon:105, reward:473.0\n",
      "epsilon:106, reward:462.0\n",
      "epsilon:107, reward:299.0\n",
      "epsilon:108, reward:199.0\n",
      "epsilon:109, reward:576.0\n",
      "epsilon:110, reward:280.0\n",
      "epsilon:111, reward:217.0\n",
      "epsilon:112, reward:323.0\n",
      "epsilon:113, reward:171.0\n",
      "epsilon:114, reward:289.0\n",
      "epsilon:115, reward:271.0\n",
      "epsilon:116, reward:210.0\n",
      "epsilon:117, reward:230.0\n",
      "epsilon:118, reward:197.0\n",
      "epsilon:119, reward:212.0\n",
      "epsilon:120, reward:180.0\n",
      "epsilon:121, reward:155.0\n",
      "epsilon:122, reward:176.0\n",
      "epsilon:123, reward:177.0\n",
      "epsilon:124, reward:161.0\n",
      "epsilon:125, reward:269.0\n",
      "epsilon:126, reward:248.0\n",
      "epsilon:127, reward:164.0\n",
      "epsilon:128, reward:139.0\n",
      "epsilon:129, reward:139.0\n",
      "epsilon:130, reward:147.0\n",
      "epsilon:131, reward:169.0\n",
      "epsilon:132, reward:164.0\n",
      "epsilon:133, reward:211.0\n",
      "epsilon:134, reward:154.0\n",
      "epsilon:135, reward:118.0\n",
      "epsilon:136, reward:158.0\n",
      "epsilon:137, reward:196.0\n",
      "epsilon:138, reward:455.0\n",
      "epsilon:139, reward:244.0\n",
      "epsilon:140, reward:211.0\n",
      "epsilon:141, reward:310.0\n",
      "epsilon:142, reward:311.0\n",
      "epsilon:143, reward:216.0\n",
      "epsilon:144, reward:444.0\n",
      "epsilon:145, reward:236.0\n",
      "epsilon:146, reward:462.0\n",
      "epsilon:147, reward:367.0\n",
      "epsilon:148, reward:289.0\n",
      "epsilon:149, reward:401.0\n",
      "epsilon:150, reward:337.0\n",
      "epsilon:151, reward:182.0\n",
      "epsilon:152, reward:677.0\n",
      "epsilon:153, reward:101.0\n",
      "epsilon:156, reward:2497.0\n",
      "epsilon:158, reward:1663.0\n",
      "epsilon:159, reward:718.0\n",
      "epsilon:160, reward:864.0\n",
      "epsilon:161, reward:683.0\n",
      "epsilon:162, reward:480.0\n",
      "epsilon:163, reward:341.0\n",
      "epsilon:164, reward:734.0\n",
      "epsilon:165, reward:629.0\n",
      "epsilon:166, reward:762.0\n",
      "epsilon:168, reward:1749.0\n",
      "epsilon:169, reward:359.0\n",
      "epsilon:170, reward:391.0\n",
      "epsilon:171, reward:525.0\n",
      "epsilon:172, reward:618.0\n",
      "epsilon:173, reward:653.0\n",
      "epsilon:174, reward:628.0\n",
      "epsilon:175, reward:508.0\n",
      "epsilon:176, reward:532.0\n",
      "epsilon:177, reward:730.0\n",
      "epsilon:178, reward:846.0\n",
      "epsilon:179, reward:970.0\n",
      "epsilon:180, reward:337.0\n",
      "epsilon:181, reward:779.0\n",
      "epsilon:182, reward:571.0\n",
      "epsilon:184, reward:1866.0\n",
      "epsilon:186, reward:1829.0\n",
      "epsilon:188, reward:1289.0\n",
      "epsilon:190, reward:1910.0\n",
      "epsilon:191, reward:782.0\n",
      "epsilon:192, reward:570.0\n",
      "epsilon:193, reward:388.0\n",
      "epsilon:194, reward:515.0\n",
      "epsilon:195, reward:343.0\n",
      "epsilon:196, reward:353.0\n",
      "epsilon:197, reward:368.0\n",
      "epsilon:198, reward:445.0\n",
      "epsilon:199, reward:335.0\n",
      "epsilon:200, reward:361.0\n",
      "epsilon:201, reward:448.0\n",
      "epsilon:202, reward:618.0\n",
      "epsilon:203, reward:396.0\n",
      "epsilon:205, reward:1409.0\n",
      "epsilon:207, reward:1500.0\n",
      "epsilon:208, reward:55.0\n",
      "epsilon:209, reward:513.0\n",
      "epsilon:210, reward:515.0\n",
      "epsilon:211, reward:318.0\n",
      "epsilon:212, reward:28.0\n",
      "epsilon:213, reward:401.0\n",
      "epsilon:214, reward:464.0\n",
      "epsilon:215, reward:520.0\n",
      "epsilon:216, reward:373.0\n",
      "epsilon:217, reward:307.0\n",
      "epsilon:218, reward:450.0\n",
      "epsilon:219, reward:564.0\n",
      "epsilon:220, reward:680.0\n",
      "epsilon:221, reward:355.0\n",
      "epsilon:222, reward:465.0\n",
      "epsilon:223, reward:958.0\n",
      "epsilon:224, reward:662.0\n",
      "epsilon:227, reward:2908.0\n",
      "epsilon:228, reward:590.0\n",
      "epsilon:230, reward:1607.0\n",
      "epsilon:232, reward:1378.0\n",
      "epsilon:233, reward:855.0\n",
      "epsilon:235, reward:1033.0\n",
      "epsilon:236, reward:915.0\n",
      "epsilon:238, reward:1342.0\n",
      "epsilon:241, reward:2834.0\n",
      "epsilon:250, reward:8630.0\n",
      "epsilon:259, reward:8572.0\n",
      "epsilon:262, reward:2947.0\n",
      "epsilon:263, reward:890.0\n",
      "epsilon:273, reward:9147.0\n",
      "epsilon:276, reward:2535.0\n",
      "epsilon:279, reward:2746.0\n",
      "epsilon:281, reward:1026.0\n",
      "epsilon:294, reward:12914.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/0p/g6nj5mqd3xl8lx0bpcg9f4nw0000gn/T/ipykernel_39734/3341989770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/0p/g6nj5mqd3xl8lx0bpcg9f4nw0000gn/T/ipykernel_39734/1848875203.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 967\u001b[0;31m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[1;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[1;32m    969\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;31m# or if the call is a partial run that specifies feeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1190\u001b[0;31m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0m\u001b[1;32m   1191\u001b[0m                              feed_dict_tensor, options, run_metadata)\n\u001b[1;32m   1192\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0m\u001b[1;32m   1369\u001b[0m                            run_metadata)\n\u001b[1;32m   1370\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1373\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1374\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1375\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1376\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1377\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0m\u001b[1;32m   1360\u001b[0m                                       target_list, run_metadata)\n\u001b[1;32m   1361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1449\u001b[0m   def _call_tf_sessionrun(self, options, feed_dict, fetch_list, target_list,\n\u001b[1;32m   1450\u001b[0m                           run_metadata):\n\u001b[0;32m-> 1451\u001b[0;31m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[0m\u001b[1;32m   1452\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m                                             run_metadata)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#env.render()\n",
    "for ep in range(EPSILON_NUM):\n",
    "    observation = env.reset()\n",
    "    index = 0\n",
    "    while True:\n",
    "        action = model.predict(observation)\n",
    "        observation, reward, done, info = env.step(action)\n",
    "        index += 1\n",
    "        model.store_transition(observation, action, reward)\n",
    "        if done:\n",
    "            print(f\"epsilon:{ep}, reward:{sum(model.ep_rs)}\")\n",
    "            model.train()\n",
    "            break\n",
    "        if index > THRESHOLD:\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407f50c4",
   "metadata": {},
   "source": [
    "## Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f5f8a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
